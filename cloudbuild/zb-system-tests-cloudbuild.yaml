substitutions:
  _REGION: "us-central1"
  _ZONE: "us-central1-a"
  _SHORT_BUILD_ID: ${BUILD_ID:0:8}

steps:

  # Step 1 Create a GCE VM to run the tests.
  # The VM is created in the same zone as the buckets to test rapid storage features.
  # It's given the 'cloud-platform' scope to allow it to access GCS and other services.
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "create-vm"
    entrypoint: "gcloud"
    args:
      - "compute"
      - "instances"
      - "create"
      - "gcsfs-test-vm-${_SHORT_BUILD_ID}"
      - "--project=${PROJECT_ID}"
      - "--zone=${_ZONE}"
      - "--machine-type=e2-medium"
      - "--image-family=debian-13"
      - "--image-project=debian-cloud"
      - "--service-account=${_ZONAL_VM_SERVICE_ACCOUNT}"
      - "--scopes=https://www.googleapis.com/auth/cloud-platform" # Full access to project APIs
      - "--metadata=enable-oslogin=TRUE"
    waitFor: ["-"]

  # Step 2: Run the integration tests inside the newly created VM and cleanup.
  # This step uses 'gcloud compute ssh' to execute a remote script.
  # The VM is deleted after tests are run, regardless of success.
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "run-tests-and-delete-vm"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        set -e
        # Wait for the VM to be fully initialized and SSH to be ready.
        for i in {1..10}; do
          if gcloud compute ssh gcsfs-test-vm-${_SHORT_BUILD_ID} --zone=${_ZONE} --internal-ip --command="echo VM is ready"; then
            break
          fi
          echo "Waiting for VM to become available... (attempt $i/10)"
          sleep 15
        done

        # Script to be executed on the VM.
        # This script installs dependencies, sets environment variables, and runs pytest.
        VM_SCRIPT="
          set -e
          echo '--- Installing git and cloning repository on VM ---'
          sudo apt-get update
          sudo apt-get install -y git python3-pip python3-venv fuse fuse3 libfuse2

          # Clone the repository and checkout the specific commit from the build trigger.
          git clone https://github.com/googleapis/python-storage.git
          cd python-storage
          git checkout ${COMMIT_SHA}


          echo '--- Installing Python and dependencies on VM ---'
          python3 -m venv env
          source env/bin/activate

          pip install --upgrade pip
          # Install testing libraries explicitly, as they are not in setup.py
          pip install pytest pytest-timeout pytest-subtests pytest-asyncio fusepy
          pip install google-cloud-testutils google-cloud-kms
          pip install -e .

          echo '--- Preparing test environment on VM ---'
          export GCSFS_TEST_BUCKET='gcsfs-test-standard-${_SHORT_BUILD_ID}'
          export GCSFS_TEST_VERSIONED_BUCKET='gcsfs-test-versioned-${_SHORT_BUILD_ID}'
          export GCSFS_ZONAL_TEST_BUCKET='${_GCSFS_ZONAL_TEST_BUCKET}'

          export STORAGE_EMULATOR_HOST=https://storage.googleapis.com
          export GCSFS_TEST_PROJECT=${PROJECT_ID}
          export GCSFS_TEST_KMS_KEY=projects/${PROJECT_ID}/locations/${_REGION}/keyRings/${_GCSFS_KEY_RING_NAME}/cryptoKeys/${_GCSFS_KEY_NAME}

          echo '--- Running Zonal tests on VM ---'
          pytest -vv -s --log-format='%(asctime)s %(levelname)s %(message)s' --log-date-format='%H:%M:%S' tests/system/test_zonal.py
        "

        # Execute the script on the VM via SSH.
        # Capture the exit code to ensure cleanup happens before the build fails.
        set +e
        gcloud compute ssh gcsfs-test-vm-${_SHORT_BUILD_ID} --zone=${_ZONE} --internal-ip --command="$$VM_SCRIPT"
        # EXIT_CODE=$?
        set -e

        echo "--- Deleting GCE VM ---"
        gcloud compute instances delete "gcsfs-test-vm-${_SHORT_BUILD_ID}" --zone=${_ZONE} --quiet

        # Exit with the original exit code from the test script.
        # exit $$EXIT_CODE
    waitFor:
      - "create-vm"

  # --- Cleanup Steps ---

  # The GCE VM is deleted in the step above to ensure cleanup happens even if tests fail.

timeout: "3600s" # 60 minutes

options:
  logging: CLOUD_LOGGING_ONLY
  pool:
    name: "projects/${PROJECT_ID}/locations/us-central1/workerPools/cloud-build-worker-pool"